{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e628738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\satvi\\anaconda3\\lib\\site-packages (from python-docx) (4.9.2)\n",
      "Collecting typing-extensions>=4.9.0 (from python-docx)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "   ---------------------------------------- 0.0/244.3 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/244.3 kB 435.7 kB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 30.7/244.3 kB 435.7 kB/s eta 0:00:01\n",
      "   ------ -------------------------------- 41.0/244.3 kB 245.8 kB/s eta 0:00:01\n",
      "   -------------- ------------------------ 92.2/244.3 kB 438.1 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 112.6/244.3 kB 504.4 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 112.6/244.3 kB 504.4 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 143.4/244.3 kB 448.2 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 194.6/244.3 kB 537.4 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 225.3/244.3 kB 551.4 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 225.3/244.3 kB 551.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- 244.3/244.3 kB 500.2 kB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, python-docx\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "Successfully installed python-docx-1.1.2 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d1157e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\satvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\satvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>TF-IDF Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>complex</td>\n",
       "      <td>0.332182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jahan</td>\n",
       "      <td>0.249136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mahal</td>\n",
       "      <td>0.498273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mausoleum</td>\n",
       "      <td>0.332182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>million</td>\n",
       "      <td>0.249136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mughal</td>\n",
       "      <td>0.332182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>shah</td>\n",
       "      <td>0.249136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>taj</td>\n",
       "      <td>0.332182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tomb</td>\n",
       "      <td>0.249136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>world</td>\n",
       "      <td>0.249136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Keyword  TF-IDF Score\n",
       "0    complex      0.332182\n",
       "1      jahan      0.249136\n",
       "2      mahal      0.498273\n",
       "3  mausoleum      0.332182\n",
       "4    million      0.249136\n",
       "5     mughal      0.332182\n",
       "6       shah      0.249136\n",
       "7        taj      0.332182\n",
       "8       tomb      0.249136\n",
       "9      world      0.249136"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "The Taj Mahal complex is believed to have been completed in its entirety in 1653 at a cost estimated at the time to be around ₹5 million, which in 2023 would be approximately ₹35 billion (US$77.8 million). The tomb is the centrepiece of a 17-hectare (42-acre) complex, which includes a mosque and a guest house, and is set in formal gardens bounded on three sides by a crenellated wall. While the mausoleum is constructed of white marble inlaid with semi-precious stones, red sandstone was used for other buildings in the complex similar to the Mughal era buildings of the time. The construction project employed more than 20,000 workers and artisans under the guidance of a board of architects led by Ustad Ahmad Lahori, the emperor's court architect. It was commissioned in 1631 by the fifth Mughal emperor, Shah Jahan (r. 1628–1658) to house the tomb of his beloved wife, Mumtaz Mahal; it also houses the tomb of Shah Jahan himself.\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import heapq\n",
    "import pandas as pd\n",
    "import docx  # Library to read Word documents\n",
    "\n",
    "# Downloading required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Step 2: Keyword Extraction using TF-IDF\n",
    "def extract_keywords(text):\n",
    "    # Vectorizing the text using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_features=10)\n",
    "    X = vectorizer.fit_transform([text])\n",
    "    \n",
    "    # Get feature names (keywords) and their scores\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = X.toarray()[0]\n",
    "    \n",
    "    # Creating a DataFrame for better visualization\n",
    "    df = pd.DataFrame({'Keyword': feature_names, 'TF-IDF Score': tfidf_scores})\n",
    "    return df\n",
    "\n",
    "# Step 3: Text Summarization\n",
    "def summarize_text(text, num_sentences=3):\n",
    "    # Tokenize sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Compute the TF-IDF matrix for the sentences\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    # Rank sentences based on the sum of TF-IDF scores\n",
    "    sentence_scores = X.sum(axis=1)\n",
    "    \n",
    "    # Select the top 'num_sentences' sentences with the highest scores\n",
    "    top_sentence_indices = heapq.nlargest(num_sentences, range(len(sentence_scores)), key=sentence_scores.__getitem__)\n",
    "    \n",
    "    # Return the summary\n",
    "    summary = [sentences[i] for i in top_sentence_indices]\n",
    "    return ' '.join(summary)\n",
    "\n",
    "\n",
    "# Function to read text from .docx file\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Reading content from the uploaded .doc file (converted to .docx)\n",
    "    file_path = \"text.docx\"  # Make sure the file is in the appropriate format\n",
    "    content = read_docx(file_path)\n",
    "\n",
    "    # Preprocessing text\n",
    "    cleaned_text = preprocess_text(content)\n",
    "\n",
    "    # Extracting keywords\n",
    "    keywords_df = extract_keywords(cleaned_text)\n",
    "    print(\"Extracted Keywords:\")\n",
    "    display(keywords_df)\n",
    "\n",
    "    # Summarizing text\n",
    "    summary = summarize_text(content, num_sentences=5)\n",
    "    print(\"\\nSummary:\")\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa02e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
